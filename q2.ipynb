{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Full Text-Driven Segmentation: CLIPSeg + SAM2\n",
        "# ==========================\n",
        "\n",
        "# ⿡ Install dependencies (run in terminal)\n",
        "!pip install torch torchvision transformers matplotlib yacs timm opencv-python Pillow\n"
      ],
      "metadata": {
        "id": "8k4xdDL0F_YT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ⿢ Imports\n",
        "import torch\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import CLIPSegProcessor, CLIPSegForImageSegmentation\n",
        "\n"
      ],
      "metadata": {
        "id": "OBNX1lQvduJo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SAM2 imports\n",
        "from sam2.build_sam import build_sam2\n",
        "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
        "\n"
      ],
      "metadata": {
        "id": "Tdld3B9yd2Sl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ⿣ Settings\n",
        "# ==========================\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Paths\n",
        "sam2_cfg = \"configs/sam2/sam2_hiera_base.yaml\"  # Relative path inside SAM2 repo\n",
        "sam2_ckpt = \"sam2_hiera_base.pt\"               # SAM2 checkpoint downloaded locally\n",
        "image_path = \"my_image.jpg\"                    # Replace with your local image\n",
        "text_prompt = \"dog\"                             # Replace with desired object\n"
      ],
      "metadata": {
        "id": "nN8fbjchd4Ac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ⿤ Load image\n",
        "# ==========================\n",
        "image = Image.open(image_path).convert(\"RGB\")\n",
        "image_np = np.array(image)\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.imshow(image)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Original Image\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "04WQXDcid6uq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ⿥ CLIPSeg: text → coarse mask\n",
        "# ==========================\n",
        "processor = CLIPSegProcessor.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n",
        "clipseg_model = CLIPSegForImageSegmentation.from_pretrained(\"CIDAS/clipseg-rd64-refined\").to(device)\n",
        "\n",
        "inputs = processor(text=[text_prompt], images=image, return_tensors=\"pt\").to(device)\n",
        "with torch.no_grad():\n",
        "    outputs = clipseg_model(**inputs)\n",
        "\n",
        "mask_coarse = torch.sigmoid(outputs.logits).squeeze().cpu().numpy()\n",
        "\n",
        "# Visualize CLIPSeg mask\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.imshow(image)\n",
        "plt.imshow(mask_coarse, alpha=0.5, cmap=\"Reds\")\n",
        "plt.axis(\"off\")\n",
        "plt.title(f\"CLIPSeg Mask: {text_prompt}\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "xnICdtmId8ys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ⿦ Convert CLIPSeg mask → bounding box\n",
        "# ==========================\n",
        "y, x = np.where(mask_coarse > 0.5)\n",
        "if len(x) == 0 or len(y) == 0:\n",
        "    input_box = np.array([0, 0, image_np.shape[1]-1, image_np.shape[0]-1])\n",
        "else:\n",
        "    input_box = np.array([min(x), min(y), max(x), max(y)])\n",
        "\n",
        "print(\"Bounding box from CLIPSeg mask:\", input_box)\n",
        "\n"
      ],
      "metadata": {
        "id": "Pr2bnD7zeCT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ⿧ SAM2: refined segmentation\n",
        "# ==========================\n",
        "sam2_model = build_sam2(sam2_cfg, sam2_ckpt, device=device)\n",
        "predictor = SAM2ImagePredictor(sam2_model)\n",
        "predictor.set_image(image_np)\n",
        "\n",
        "# Predict refined mask using bounding box\n",
        "sam_mask, _, _ = predictor.predict(box=input_box[None, :])\n",
        "mask_refined = sam_mask[0][0]\n",
        "\n",
        "# Visualize SAM2 refined mask\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.imshow(image_np)\n",
        "plt.imshow(mask_refined, alpha=0.5, cmap=\"Reds\")\n",
        "plt.axis(\"off\")\n",
        "plt.title(f\"SAM2 Refined Mask: {text_prompt}\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-cqEfC3GeEOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Uf0w03vpeGIZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}