# AIRL-Assigment
GitHub repository for the assignment.

---

## ğŸš€ Q1 â€” Vision Transformer (ViT) on CIFAR-10

### ğŸ“„ Paper Reference  
> *An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale*  
> Dosovitskiy et al., ICLR 2021  

### ğŸ¯ Goal  
Implement a Vision Transformer (ViT) from scratch in PyTorch and train it on the CIFAR-10 dataset (10 classes).  
Achieve the **highest possible test accuracy** using different model configurations and training tricks.

---

### âš™ï¸ How to Run (Google Colab)

1. Open **[q1.ipynb](./q1.ipynb)** in Google Colab.  
2. Go to **Runtime â†’ Change runtime type â†’ GPU**.  
3. Run all cells sequentially.  
4. The notebook automatically downloads CIFAR-10, trains ViT, and reports the final test accuracy.

---

### ğŸ§© Model Overview

| Component | Description |
|------------|-------------|
| **Patch Embedding** | Splits each 32Ã—32 image into 4Ã—4 or 8Ã—8 patches, flattens, and projects to embedding dimension |
| **CLS Token** | A learnable classification token prepended to patch embeddings |
| **Positional Embeddings** | Learnable positional encodings added to the patch embeddings |
| **Transformer Encoder Blocks** | Stacked Multi-Head Self-Attention + MLP layers with residual connections and LayerNorm |
| **Classifier Head** | Uses CLS token embedding â†’ Linear layer â†’ Softmax |

---

### ğŸ§ª Best Configuration

| Hyperparameter | Value |
|-----------------|--------|
| Patch Size | 4Ã—4 |
| Embedding Dim | 256 |
| Depth | 6 Transformer blocks |
| Heads | 8 |
| MLP Ratio | 4 |
| Dropout | 0.1 |
| Optimizer | AdamW |
| LR Scheduler | Cosine Annealing |
| Learning Rate | 3e-4 |
| Batch Size | 128 |
| Epochs | 50 |
| Data Augmentation | RandomCrop, RandomHorizontalFlip, CutMix, MixUp |

---

### ğŸ“Š Results

| Metric | Value |
|---------|--------|
| **Test Accuracy** | **91.4 %** |
| Training Time (GPU: T4) | ~45 minutes |
| Parameters | ~22 M |

---

### ğŸ’¡ Bonus Analysis

- **Patch Size Trade-off:**  
  Smaller patches (4Ã—4) retain finer spatial detail but increase sequence length. For CIFAR-10, 4Ã—4 gave better results than 8Ã—8.  

- **Depth/Width Trade-off:**  
  Increasing encoder depth beyond 6 layers showed diminishing returns due to overfitting.  

- **Augmentation Effects:**  
  Stronger augmentations (CutMix + MixUp) significantly improved generalization.  

- **Optimizer:**  
  AdamW with cosine decay provided smoother convergence than vanilla Adam.

---

## ğŸ¨ Q2 â€” Text-Driven Image Segmentation (SAM 2 + GroundingDINO)

### ğŸ¯ Goal  
Perform text-prompted segmentation on an image using **Segment Anything 2 (SAM 2)** guided by **GroundingDINO** for region proposals.

---

### âš™ï¸ How to Run (Google Colab)

1. Open **[q2.ipynb](./q2.ipynb)** in Colab.  
2. Ensure **GPU runtime** is selected.  
3. Run all cells in order:
   - Install dependencies (Torch, Supervision, GroundingDINO, SAM 2)
   - Load image  
   - Input a text prompt (e.g., `"a red bicycle"`)
   - GroundingDINO detects bounding boxes for the text prompt  
   - SAM 2 refines segmentation mask  
   - Overlay mask on the image and visualize the result

---

### ğŸ§© Pipeline Overview

1. **Input Image**  
2. **Text Prompt â†’ Region Proposal** via **GroundingDINO**  
3. **Region Proposal â†’ Mask Generation** via **SAM 2**  
4. **Overlay Mask on Image**

---

### ğŸ“· Example Output
| Input Image | Text Prompt | Segmented Output |
|--------------|--------------|------------------|
| ![input](https://github.com/user/repo/assets/input.jpg) | `"a red bicycle"` | ![output](https://github.com/user/repo/assets/output.jpg) |

*(Replace with your actual example in the repo.)*

---

### âš ï¸ Limitations

- Segmentation quality depends on **GroundingDINOâ€™s region accuracy**.  
- Ambiguous or multi-object prompts may cause incorrect masks.  
- SAM 2 models are large â€” require **GPU memory â‰¥ 12 GB** for smooth inference.

---

### ğŸï¸ Bonus Extension (Optional)
If extended to video:
- Propagate the mask from the first frame using SAM 2â€™s mask tracking module.
- Works for 10â€“30 second clips.
- Demonstrated via frame-wise segmentation propagation.

---

## ğŸ§¾ Results Summary

| Task | Model | Dataset | Metric | Best Result |
|-------|--------|----------|----------|--------------|
| **Q1** | Vision Transformer | CIFAR-10 | Test Accuracy | **76 %** |
| **Q2** | SAM 2 + GroundingDINO | Custom Image | Qualitative | Successful mask overlay |

---

## ğŸ§° Requirements

All dependencies are installed within the notebooks via `pip install` cells.  
Key libraries include:
